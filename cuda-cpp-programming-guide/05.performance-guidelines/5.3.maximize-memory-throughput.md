# 5.3 메모리 처리량 최대화

애플리케이션의 전체 메모리 처리량을 최대화하기 위한 첫 번째 단계는 낮은 대역폭을 가진 데이터 전송을 최소화하는 것입니다.

이는 호스트와 디바이스 간의 데이터 전송을 최소화하는 것을 의미합니다. [호스트와 디바이스 간의 데이터 전송](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#data-transfer-between-host-and-device)은 전역 메모리와 디바이스 간의 데이터 전송보다 훨씬 낮은 대역폭을 가지기 때문입니다.

또한 온칩 메모리(공유 메모리와 캐시)의 사용을 최대화하여 전역 메모리와 디바이스 간의 데이터 전송을 최소화해야 합니다. 여기서 캐시는 컴퓨트 능력 2.x 이상의 디바이스에서 사용 가능한 L1 캐시와 L2 캐시, 그리고 모든 디바이스에서 사용 가능한 텍스처 캐시와 상수 캐시를 포함합니다.

공유 메모리는 사용자가 관리하는 캐시와 같습니다: 애플리케이션이 명시적으로 할당하고 접근합니다. [CUDA 런타임](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#cuda-c-runtime)에서 설명한 것처럼, 일반적인 프로그래밍 패턴은 디바이스 메모리에서 공유 메모리로 데이터를 스테이징하는 것입니다. 즉, 블록의 각 스레드가 다음과 같은 작업을 수행합니다:

- 디바이스 메모리에서 공유 메모리로 데이터 로드
- 다른 스레드들이 채운 공유 메모리 위치를 안전하게 읽을 수 있도록 블록의 모든 다른 스레드와 동기화
- 공유 메모리에서 데이터 처리
- 필요한 경우 공유 메모리가 결과로 업데이트되었는지 확인하기 위해 다시 동기화
- 결과를 디바이스 메모리로 다시 쓰기

일부 애플리케이션의 경우(예: 전역 메모리 접근 패턴이 데이터에 종속적인 경우), 전통적인 하드웨어 관리 캐시가 데이터 지역성을 활용하는 데 더 적합합니다. 컴퓨트 능력 7.x, 8.x 및 9.0에서 언급된 것처럼, 이러한 컴퓨트 능력을 가진 디바이스에서는 동일한 온칩 메모리가 L1과 공유 메모리 모두에 사용되며, 각 커널 호출마다 L1과 공유 메모리에 얼마나 많은 메모리를 할당할지 구성할 수 있습니다.

커널의 메모리 접근 처리량은 각 메모리 유형에 대한 접근 패턴에 따라 크게 달라질 수 있습니다. 따라서 메모리 처리량을 최대화하기 위한 다음 단계는 디바이스 메모리 접근에서 설명하는 최적의 메모리 접근 패턴을 기반으로 메모리 접근을 최대한 최적화하는 것입니다. 이 최적화는 특히 전역 메모리 접근에 중요합니다. 전역 메모리 대역폭은 사용 가능한 온칩 대역폭과 산술 명령어 처리량에 비해 낮기 때문에, 최적화되지 않은 전역 메모리 접근은 일반적으로 성능에 큰 영향을 미칩니다.

## 5.3.1 호스트와 디바이스 간의 데이터 전송

애플리케이션은 호스트와 디바이스 간의 데이터 전송을 최소화하도록 노력해야 합니다. 이를 달성하는 한 가지 방법은 디바이스에서 완전한 효율성으로 실행하기에 충분한 병렬성을 노출하지 않더라도, 호스트에서 디바이스로 더 많은 코드를 이동하는 것입니다. 중간 데이터 구조는 디바이스 메모리에서 생성되어 디바이스에 의해 처리되고, 호스트에 의해 매핑되거나 호스트 메모리로 복사되지 않고도 파괴될 수 있습니다.

또한, 각 전송과 관련된 오버헤드 때문에, 많은 작은 전송을 하나의 큰 전송으로 배치하는 것이 각 전송을 개별적으로 수행하는 것보다 항상 더 나은 성능을 보입니다.

프론트사이드 버스가 있는 시스템에서는 [페이지-잠금 호스트 메모리](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#page-locked-host-memory)를 사용하여 호스트와 디바이스 간의 데이터 전송에서 더 높은 성능을 얻을 수 있습니다.

또한, 매핑된 페이지-잠금 메모리 ([Mapped Memory](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#mapped-memory))를 사용할 때는 디바이스 메모리를 할당하고 디바이스와 호스트 메모리 간에 명시적으로 데이터를 복사할 필요가 없습니다. 데이터 전송은 커널이 매핑된 메모리에 접근할 때마다 암시적으로 수행됩니다. 최대 성능을 위해서는 이러한 메모리 접근이 전역 메모리 접근과 마찬가지로 병합되어야 합니다([Device Memory Accesses](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#device-memory-accesses) 참조). 이러한 조건이 충족되고 매핑된 메모리가 한 번만 읽거나 쓰여진다고 가정하면, 디바이스와 호스트 메모리 간의 명시적 복사 대신 매핑된 페이지-잠금 메모리를 사용하는 것이 성능상 이점이 될 수 있습니다.

디바이스 메모리와 호스트 메모리가 물리적으로 동일한 통합 시스템에서는 호스트와 디바이스 메모리 간의 복사가 불필요하며 대신 매핑된 페이지-잠금 메모리를 사용해야 합니다. 애플리케이션은 통합 디바이스 속성([Device Enumeration](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#device-enumeration) 참조)이 1인지 확인하여 디바이스가 통합되어 있는지 확인할 수 있습니다.

## 5.3.2 디바이스 메모리 접근

주소 지정 가능한 메모리(즉, 전역, 로컬, 공유, 상수 또는 텍스처 메모리)에 접근하는 명령어는 워프 내의 스레드 간 메모리 주소의 분포에 따라 여러 번 재발행되어야 할 수 있습니다. 이러한 분포가 명령어 처리량에 미치는 영향은 각 메모리 유형에 따라 다르며 다음 섹션에서 설명합니다. 예를 들어, 전역 메모리의 경우 일반적으로 주소가 더 분산될수록 처리량이 더 감소합니다.

### 전역 메모리

전역 메모리는 디바이스 메모리에 있으며 디바이스 메모리는 32-, 64- 또는 128바이트 메모리 트랜잭션을 통해 접근됩니다. 이러한 메모리 트랜잭션은 자연스럽게 정렬되어야 합니다: 크기에 맞게 정렬된 디바이스 메모리의 32-, 64- 또는 128바이트 세그먼트(즉, 첫 번째 주소가 해당 크기의 배수인 세그먼트)만 메모리 트랜잭션에 의해 읽거나 쓸 수 있습니다.

워프가 전역 메모리에 접근하는 명령어를 실행할 때, 각 스레드가 접근하는 워드의 크기와 스레드 간의 메모리 주소 분포에 따라 워프 내 스레드의 메모리 접근을 하나 이상의 이러한 메모리 트랜잭션으로 병합합니다. 일반적으로 필요한 트랜잭션이 많을수록 스레드가 접근하는 워드 외에도 더 많은 미사용 워드가 전송되어 그에 따라 명령어 처리량이 감소합니다. 예를 들어, 각 스레드의 4바이트 접근에 대해 32바이트 메모리 트랜잭션이 생성되면 처리량은 8로 나뉩니다.

필요한 트랜잭션 수와 최종적으로 처리량에 미치는 영향은 디바이스의 컴퓨트 능력에 따라 다릅니다. [컴퓨트 능력 5.x](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#compute-capability-5-x), [6.x](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#compute-capability-6-x), [7.x](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#compute-capability-7-x), [8.x](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#compute-capability-8-x) 및 [9.0](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#compute-capability-9-0)은 다양한 컴퓨트 능력에 대해 전역 메모리 접근이 어떻게 처리되는지에 대한 자세한 내용을 제공합니다.

전역 메모리 처리량을 최대화하기 위해서는 다음과 같이 병합을 최대화하는 것이 중요합니다:

- 컴퓨트 능력 5.x, 6.x, 7.x, 8.x 및 9.0을 기반으로 가장 최적의 접근 패턴을 따르기
- 아래의 크기 및 정렬 요구사항 섹션에서 설명하는 요구사항을 충족하는 데이터 유형 사용
- 일부 경우에는 데이터 패딩, 예를 들어 아래의 2차원 배열 섹션에서 설명하는 것처럼 2차원 배열에 접근할 때

### 크기 및 정렬 요구사항

전역 메모리 명령어는 1, 2, 4, 8 또는 16바이트 크기의 워드를 읽거나 쓰는 것을 지원합니다. 전역 메모리에 있는 데이터에 대한 접근(변수 또는 포인터를 통해)은 데이터 유형의 크기가 1, 2, 4, 8 또는 16바이트이고 데이터가 자연스럽게 정렬된 경우(즉, 주소가 해당 크기의 배수인 경우)에만 단일 전역 메모리 명령어로 컴파일됩니다.

이 크기 및 정렬 요구사항이 충족되지 않으면, 접근은 이러한 명령어가 완전히 병합되는 것을 방지하는 인터리브된 접근 패턴을 가진 여러 명령어로 컴파일됩니다. 따라서 전역 메모리에 있는 데이터에 대해서는 이 요구사항을 충족하는 유형을 사용하는 것이 권장됩니다.

정렬 요구사항은 [내장 벡터 유형](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#built-in-vector-types)에 대해 자동으로 충족됩니다.

구조체의 경우, 크기 및 정렬 요구사항은 다음과 같이 `__align__(8)` 또는 `__align__(16)` 정렬 지정자를 사용하여 컴파일러에 의해 강제될 수 있습니다:

```cpp
struct __align__(8) {
    float x;
    float y;
};
```
또는
```cpp
struct __align__(16) {
    float x;
    float y;
    float z;
};
```

전역 메모리에 있는 변수의 주소나 드라이버 또는 런타임 API의 메모리 할당 루틴에서 반환된 주소는 항상 최소 256바이트에 정렬됩니다.

자연스럽게 정렬되지 않은 8바이트 또는 16바이트 워드를 읽으면 잘못된 결과(몇 개의 워드만큼 차이가 남)가 생성되므로, 이러한 유형의 값이나 값 배열의 시작 주소의 정렬을 유지하는 데 특별한 주의를 기울여야 합니다. 이러한 문제가 쉽게 간과될 수 있는 일반적인 경우는 사용자 정의 전역 메모리 할당 체계를 사용할 때입니다. 여기서 여러 배열의 할당(`cudaMalloc()` 또는 `cuMemAlloc()`에 대한 여러 호출)이 여러 배열로 분할된 단일 큰 메모리 블록의 할당으로 대체되는 경우, 각 배열의 시작 주소가 블록의 시작 주소에서 오프셋됩니다.

### 2차원 배열

일반적인 전역 메모리 접근 패턴은 인덱스 `(tx,ty)`를 가진 각 스레드가 다음 주소를 사용하여 너비가 `width`인 2D 배열의 한 요소에 접근하는 경우입니다. 이 배열은 `type*` 유형의 `BaseAddress` 주소에 위치합니다(여기서 `type`은 [활용 최대화](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#maximize-utilization)에서 설명한 요구사항을 충족합니다):

```cpp
BaseAddress + width * ty + tx
```

이러한 접근이 완전히 병합되려면 스레드 블록의 너비와 배열의 너비가 모두 워프 크기의 배수여야 합니다.

특히, 이는 너비가 이 크기의 배수가 아닌 배열이 이 크기의 가장 가까운 배수로 반올림된 너비로 실제로 할당되고 그에 따라 행이 패딩된 경우 훨씬 더 효율적으로 접근될 수 있다는 것을 의미합니다. 참조 매뉴얼에서 설명하는 `cudaMallocPitch()` 및 `cuMemAllocPitch()` 함수와 관련 메모리 복사 함수를 사용하면 프로그래머가 이러한 제약 조건을 준수하는 배열을 할당하는 하드웨어 독립적인 코드를 작성할 수 있습니다.

### 로컬 메모리

로컬 메모리 접근은 [변수 메모리 공간 지정자](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#variable-memory-space-specifiers)에서 언급한 것처럼 일부 자동 변수에 대해서만 발생합니다. 컴파일러가 로컬 메모리에 배치할 가능성이 있는 자동 변수는 다음과 같습니다:

- 상수 양으로 인덱싱된다는 것을 확인할 수 없는 배열
- 너무 많은 레지스터 공간을 소비하는 큰 구조체나 배열
- 커널이 사용 가능한 것보다 더 많은 레지스터를 사용하는 경우의 모든 변수(이를 *레지스터 스필링*이라고도 함)

*PTX* 어셈블리 코드 검사(`-ptx` 또는 `-keep` 옵션으로 컴파일하여 얻음)를 통해 변수가 첫 번째 컴파일 단계에서 로컬 메모리에 배치되었는지 알 수 있습니다. 이 경우 `.local` 니모닉(mnemonic)으로 선언되고 `ld.local` 및 `st.local` 니모닉을 사용하여 접근됩니다. 그렇지 않더라도 후속 컴파일 단계에서 대상 아키텍처에 대해 너무 많은 레지스터 공간을 소비한다고 판단하면 다르게 결정할 수 있습니다: `cuobjdump`를 사용한 *cubin* 객체 검사를 통해 이를 알 수 있습니다. 또한 컴파일러는 `--ptxas-options=-v` 옵션으로 컴파일할 때 커널당 총 로컬 메모리 사용량(`lmem`)을 보고합니다. 일부 수학 함수는 로컬 메모리에 접근할 수 있는 구현 경로를 가지고 있습니다.

로컬 메모리 공간은 디바이스 메모리에 있으므로, 로컬 메모리 접근은 전역 메모리 접근과 동일한 높은 지연 시간과 낮은 대역폭을 가지며 [디바이스 메모리 접근](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#device-memory-accesses)에서 설명한 것과 동일한 메모리 병합 요구사항의 대상이 됩니다. 그러나 로컬 메모리는 연속적인 32비트 워드가 연속적인 스레드 ID에 의해 접근되도록 구성되어 있습니다. 따라서 워프의 모든 스레드가 동일한 상대 주소(예: 배열 변수의 동일한 인덱스, 구조체 변수의 동일한 멤버)에 접근하는 한 접근은 완전히 병합됩니다.

컴퓨트 능력 5.x 이상의 디바이스에서는 로컬 메모리 접근이 항상 전역 메모리 접근과 동일한 방식으로 L2에 캐시됩니다.

### 공유 메모리

온칩에 있기 때문에 공유 메모리는 로컬 또는 전역 메모리보다 훨씬 높은 대역폭과 훨씬 낮은 지연 시간을 가집니다.

높은 대역폭을 달성하기 위해 공유 메모리는 동시에 접근할 수 있는 뱅크라고 하는 동일한 크기의 메모리 모듈로 나뉩니다. n개의 서로 다른 메모리 뱅크에 속하는 n개의 주소로 구성된 메모리 읽기 또는 쓰기 요청은 동시에 처리될 수 있으므로, 단일 모듈의 대역폭보다 n배 높은 전체 대역폭을 얻을 수 있습니다.

그러나 메모리 요청의 두 주소가 동일한 메모리 뱅크에 속하는 경우, 뱅크 충돌이 발생하고 접근이 직렬화되어야 합니다. 하드웨어는 뱅크 충돌이 있는 메모리 요청을 필요한 만큼의 별도의 충돌 없는 요청으로 분할하며, 이는 별도의 메모리 요청 수만큼 처리량을 감소시킵니다. 별도의 메모리 요청 수가 n인 경우, 초기 메모리 요청이 n-way 뱅크 충돌을 일으킨다고 합니다.

최대 성능을 얻기 위해서는 메모리 주소가 메모리 뱅크에 매핑되는 방식을 이해하여 뱅크 충돌을 최소화하도록 메모리 요청을 스케줄링하는 것이 중요합니다. 이는 컴퓨트 능력 5.x, 6.x, 7.x, 8.x 및 9.0의 디바이스에 대해 각각의 컴퓨트 능력 섹션에서 설명됩니다.

### 상수 메모리

상수 메모리 공간은 디바이스 메모리에 있으며 상수 캐시에 캐시됩니다.

요청은 초기 요청의 서로 다른 메모리 주소 수만큼의 별도 요청으로 분할되며, 이는 별도 요청 수만큼 처리량을 감소시킵니다.

결과 요청은 캐시 히트의 경우 상수 캐시의 처리량으로, 그렇지 않은 경우 디바이스 메모리의 처리량으로 처리됩니다.

### 텍스처 및 서피스 메모리

텍스처 및 서피스 메모리 공간은 디바이스 메모리에 있으며 텍스처 캐시에 캐시됩니다. 따라서 텍스처 페치 또는 서피스 읽기는 캐시 미스의 경우에만 디바이스 메모리에서 한 번의 메모리 읽기가 필요하며, 그렇지 않으면 텍스처 캐시에서 한 번의 읽기만 필요합니다. 텍스처 캐시는 2D 공간 지역성에 최적화되어 있어, 2D에서 가까운 텍스처 또는 서피스 주소를 읽는 동일한 워프의 스레드가 최상의 성능을 달성합니다. 또한 일정한 지연 시간으로 스트리밍 페치를 위해 설계되었습니다. 캐시 히트는 DRAM 대역폭 요구를 줄이지만 페치 지연 시간은 줄이지 않습니다.

텍스처 또는 서피스 페칭을 통해 디바이스 메모리를 읽는 것은 전역 또는 상수 메모리에서 디바이스 메모리를 읽는 것의 유리한 대안이 될 수 있는 몇 가지 이점을 제공합니다:

- 메모리 읽기가 좋은 성능을 얻기 위해 전역 또는 상수 메모리 읽기가 따라야 하는 접근 패턴을 따르지 않는 경우, 텍스처 페치 또는 서피스 읽기에 지역성이 있다면 더 높은 대역폭을 달성할 수 있습니다.
- 주소 계산이 전용 유닛에 의해 커널 외부에서 수행됩니다.
- 패킹된 데이터를 단일 연산으로 별도의 변수로 브로드캐스트할 수 있습니다.
- 8비트 및 16비트 정수 입력 데이터는 선택적으로 [0.0, 1.0] 또는 [-1.0, 1.0] 범위의 32비트 부동소수점 값으로 변환될 수 있습니다([텍스처 메모리](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#texture-memory) 참조).