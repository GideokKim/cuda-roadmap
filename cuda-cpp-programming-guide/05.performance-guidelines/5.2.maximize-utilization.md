## 5.2. 활용 극대화

활용을 극대화하기 위해 애플리케이션은 가능한 한 많은 병렬성을 노출하고 이 병렬성을 시스템의 다양한 구성 요소에 효율적으로 매핑하여 대부분의 시간을 바쁘게 유지하도록 구조화되어야 합니다.

### 5.2.1. 애플리케이션 수준

높은 수준에서 애플리케이션은 호스트, 디바이스 및 호스트와 디바이스를 연결하는 버스 간의 병렬 실행을 극대화해야 하며, 이를 위해 [비동기 동시 실행](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#asynchronous-concurrent-execution)에서 설명한 대로 비동기 함수 호출 및 스트림을 사용해야 합니다. 각 프로세서에 가장 잘 수행할 수 있는 작업 유형을 할당해야 합니다: 직렬 작업은 호스트에, 병렬 작업은 디바이스에 할당합니다.

병렬 작업의 경우, 알고리즘의 특정 지점에서 일부 스레드가 서로 데이터를 공유하기 위해 동기화해야 하므로 병렬성이 깨지는 경우가 있습니다. 이 경우 두 가지 경우가 있습니다: 이러한 스레드가 동일한 블록에 속하는 경우, `__syncthreads()`를 사용하고 동일한 커널 호출 내에서 공유 메모리를 통해 데이터를 공유해야 하며, 서로 다른 블록에 속하는 경우, 두 개의 별도 커널 호출을 통해 전역 메모리를 사용하여 데이터를 공유해야 합니다. 이 경우는 추가 커널 호출과 전역 메모리 트래픽의 오버헤드가 추가되므로 훨씬 덜 최적화됩니다. 따라서 알고리즘을 CUDA 프로그래밍 모델에 매핑하여 스레드 간 통신이 필요한 계산이 가능한 한 단일 스레드 블록 내에서 수행되도록 하여 이러한 경우의 발생을 최소화해야 합니다.

### 5.2.2. 디바이스 수준

더 낮은 수준에서 애플리케이션은 디바이스의 멀티프로세서 간의 병렬 실행을 극대화해야 합니다.

여러 커널이 디바이스에서 동시에 실행될 수 있으므로, 비동기 동시 실행에서 설명한 대로 스트림을 사용하여 충분한 커널이 동시에 실행되도록 하여 최대 활용을 달성할 수 있습니다.

### 5.2.3. 멀티프로세서 수준

더 낮은 수준에서 애플리케이션은 멀티프로세서 내의 다양한 기능 유닛 간의 병렬 실행을 극대화해야 합니다.

[하드웨어 다중 스레딩](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#hardware-multithreading)에서 설명한 바와 같이, GPU 멀티프로세서는 기능 유닛의 활용을 극대화하기 위해 주로 스레드 수준의 병렬성에 의존합니다. 따라서 활용도는 거주하는 워프의 수와 직접적으로 연결됩니다. 각 명령어 발행 시점에 워프 스케줄러는 실행할 준비가 된 명령어를 선택합니다. 이 명령어는 동일한 워프의 다른 독립 명령어일 수 있으며, 이는 명령어 수준의 병렬성을 활용하거나, 더 일반적으로는 다른 워프의 명령어일 수 있으며, 이는 스레드 수준의 병렬성을 활용합니다. 실행할 준비가 된 명령어가 선택되면 활성 스레드에 발행됩니다. 워프가 다음 명령어를 실행할 준비가 되는 데 걸리는 클럭 사이클 수를 지연(latency)이라고 하며, 모든 워프 스케줄러가 지연 기간 동안 매 클럭 사이클마다 어떤 워프에 대해 발행할 명령어가 항상 있는 경우, 즉 지연이 완전히 "숨겨질" 때 전체 활용도가 달성됩니다. L 클럭 사이클의 지연을 숨기기 위해 필요한 명령어 수는 이러한 명령어의 각각의 처리량에 따라 달라집니다(다양한 산술 명령어의 처리량은 [산술 명령어](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#arithmetic-instructions)에서 참조). 최대 처리량을 가진 명령어를 가정하면, 이는 다음과 같습니다:

- 5.x, 6.1, 6.2, 7.x 및 8.x의 컴퓨트 능력을 가진 디바이스의 경우, 이러한 디바이스에서는 멀티프로세서가 한 클럭 사이클에 네 개의 워프에 대해 하나의 명령어를 발행하므로 4L입니다.
- 6.0의 컴퓨트 능력을 가진 디바이스의 경우, 이러한 디바이스에서는 매 사이클에 발행되는 두 개의 명령어가 두 개의 서로 다른 워프에 대한 하나의 명령어이므로 2L입니다.

워프가 다음 명령어를 실행할 준비가 되지 않는 가장 일반적인 이유는 명령어의 입력 피연산자가 아직 사용 가능하지 않기 때문입니다.

모든 입력 피연산자가 레지스터인 경우, 지연은 레지스터 종속성에 의해 발생합니다. 즉, 일부 입력 피연산자는 이전 명령어에 의해 작성되었으며, 해당 명령어의 실행이 아직 완료되지 않았습니다. 이 경우 지연은 이전 명령어의 실행 시간과 같으며, 워프 스케줄러는 이 시간 동안 다른 워프의 명령어를 스케줄해야 합니다. 실행 시간은 명령어에 따라 다릅니다. 7.x의 컴퓨트 능력을 가진 디바이스에서는 대부분의 산술 명령어의 경우 일반적으로 4 클럭 사이클입니다. 이는 멀티프로세서당 16개의 활성 워프(4 사이클, 4 워프 스케줄러)가 산술 명령어 지연을 숨기기 위해 필요하다는 것을 의미합니다(워프가 최대 처리량으로 명령어를 실행한다고 가정할 경우, 그렇지 않으면 더 적은 수의 워프가 필요합니다). 개별 워프가 명령어 수준의 병렬성을 나타내는 경우, 즉 명령어 스트림에 여러 개의 독립 명령어가 있는 경우, 단일 워프에서 여러 개의 독립 명령어를 연속적으로 발행할 수 있으므로 더 적은 수의 워프가 필요합니다.

일부 입력 피연산자가 오프칩 메모리에 있는 경우, 지연은 훨씬 더 높습니다: 일반적으로 수백 클럭 사이클입니다. 이러한 높은 지연 기간 동안 워프 스케줄러를 바쁘게 유지하는 데 필요한 워프 수는 커널 코드와 그 명령어 수준의 병렬성 정도에 따라 달라집니다. 일반적으로 오프칩 메모리 피연산자가 없는 명령어 수(즉, 대부분의 경우 산술 명령어)와 오프칩 메모리 피연산자가 있는 명령어 수의 비율이 낮을수록 더 많은 워프가 필요합니다(이 비율은 일반적으로 프로그램의 산술 강도(arithmetic intensity)라고 불립니다).

워프가 다음 명령어를 실행할 준비가 되지 않는 또 다른 이유는 메모리 펜스([메모리 펜스 함수](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#memory-fence-functions)) 또는 동기화 지점([동기화 함수](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#synchronization-functions))에서 대기하고 있기 때문입니다. 동기화 지점은 멀티프로세서를 유휴 상태로 만들 수 있으며, 더 많은 워프가 동일 블록 내의 다른 워프가 동기화 지점 이전의 명령어 실행을 완료할 때까지 기다리게 됩니다. 이 경우 멀티프로세서당 여러 개의 거주 블록을 두는 것이 유휴 상태를 줄이는 데 도움이 될 수 있습니다. 왜냐하면 서로 다른 블록의 워프는 동기화 지점에서 서로 기다릴 필요가 없기 때문입니다.

주어진 커널 호출에 대해 각 멀티프로세서에 거주하는 블록과 워프의 수는 호출의 실행 구성([실행 구성](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#execution-configuration)), 멀티프로세서의 메모리 자원, 그리고 하드웨어 다중 스레딩에서 설명한 커널의 자원 요구 사항에 따라 달라집니다. 레지스터 및 공유 메모리 사용량은 `--ptxas-options=-v` 옵션으로 컴파일할 때 컴파일러에 의해 보고됩니다.

블록에 필요한 총 공유 메모리 양은 정적으로 할당된 공유 메모리의 양과 동적으로 할당된 공유 메모리의 양의 합과 같습니다.

커널에서 사용되는 레지스터 수는 거주하는 워프의 수에 상당한 영향을 미칠 수 있습니다. 예를 들어, 6.x의 컴퓨트 능력을 가진 디바이스의 경우, 커널이 64개의 레지스터를 사용하고 각 블록에 512개의 스레드가 있으며 공유 메모리가 매우 적게 필요한 경우, 두 블록(즉, 32개의 워프)이 멀티프로세서에 거주할 수 있습니다. 이는 2x512x64 레지스터를 필요로 하며, 이는 멀티프로세서에서 사용 가능한 레지스터 수와 정확히 일치합니다. 그러나 커널이 하나의 레지스터를 더 사용하게 되면, 두 블록은 2x512x65 레지스터를 필요로 하게 되어 멀티프로세서에서 사용 가능한 레지스터 수를 초과하게 됩니다. 따라서 컴파일러는 레지스터 사용량을 최소화하려고 하며, 레지스터 스필링(see [장치 메모리 접근](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#device-memory-accesses))과 명령어 수를 최소한으로 유지하려고 합니다. 레지스터 사용량은 `maxrregcount` 컴파일러 옵션, [런치 경계](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#launch-bounds)에서 설명한 `__launch_bounds__()` 한정자, 또는 [스레드당 최대 레지스터 수](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#maximum-number-of-registers-per-thread)에서 설명한 `__maxnreg__()` 한정자를 사용하여 제어할 수 있습니다.

레지스터 파일은 32비트 레지스터로 구성됩니다. 따라서 레지스터에 저장된 각 변수는 최소한 하나의 32비트 레지스터가 필요하며, 예를 들어 `double` 변수는 두 개의 32비트 레지스터를 사용합니다.

주어진 커널 호출에 대한 실행 구성의 성능에 대한 효과는 일반적으로 커널 코드에 따라 달라집니다. 따라서 실험이 권장됩니다. 애플리케이션은 또한 레지스터 파일 크기와 공유 메모리 크기에 따라 실행 구성을 매개변수화할 수 있으며, 이는 장치의 컴퓨트 능력뿐만 아니라 멀티프로세서의 수와 장치의 메모리 대역폭에 따라 달라지며, 이 모든 것은 런타임을 사용하여 쿼리할 수 있습니다(참조 매뉴얼 참조).

블록당 스레드 수는 가능한 한 적재되지 않은 워프를 피하기 위해 워프 크기의 배수로 선택해야 합니다.

#### 5.2.3.1. 점유율 계산기

레지스터 및 공유 메모리 요구 사항에 따라 프로그래머가 스레드 블록 크기와 클러스터 크기를 선택하는 데 도움을 주는 여러 API 함수가 있습니다.

- 점유율 계산기 API인 `cudaOccupancyMaxActiveBlocksPerMultiprocessor`는 커널의 블록 크기와 공유 메모리 사용량에 따라 점유율 예측을 제공할 수 있습니다. 이 함수는 멀티프로세서당 동시 스레드 블록 수 측면에서 점유율을 보고합니다.
  - 이 값은 다른 메트릭으로 변환될 수 있습니다. 블록당 워프 수를 곱하면 멀티프로세서당 동시 워프 수가 나오고, 동시 워프 수를 멀티프로세서당 최대 워프 수로 나누면 점유율이 백분율로 나타납니다.
- 점유율 기반 런치 구성 API인 `cudaOccupancyMaxPotentialBlockSize` 및 `cudaOccupancyMaxPotentialBlockSizeVariableSMem`은 최대 멀티프로세서 수준 점유율을 달성하는 실행 구성을 휴리스틱적으로 계산합니다.
- 점유율 계산기 API인 `cudaOccupancyMaxActiveClusters`는 클러스터 크기, 블록 크기 및 커널의 공유 메모리 사용량에 따라 점유율 예측을 제공할 수 있습니다. 이 함수는 시스템에 있는 GPU의 주어진 크기의 최대 활성 클러스터 수 측면에서 점유율을 보고합니다.

다음 코드 샘플은 MyKernel의 점유율을 계산합니다. 그런 다음 동시 워프와 멀티프로세서당 최대 워프 간의 비율로 점유율 수준을 보고합니다.

```cpp
// Device code
__global__ void MyKernel(int *d, int *a, int *b)
{
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    d[idx] = a[idx] * b[idx];
}

// Host code
int main()
{
    int numBlocks;        // Occupancy in terms of active blocks
    int blockSize = 32;

    // These variables are used to convert occupancy to warps
    int device;
    cudaDeviceProp prop;
    int activeWarps;
    int maxWarps;

    cudaGetDevice(&device);
    cudaGetDeviceProperties(&prop, device);

    cudaOccupancyMaxActiveBlocksPerMultiprocessor(
        &numBlocks,
        MyKernel,
        blockSize,
        0);

    activeWarps = numBlocks * blockSize / prop.warpSize;
    maxWarps = prop.maxThreadsPerMultiProcessor / prop.warpSize;

    std::cout << "Occupancy: " << (double)activeWarps / maxWarps * 100 << "%" << std::endl;

    return 0;
}
```

다음 코드 샘플은 사용자 입력에 따라 MyKernel의 점유율 기반 커널 실행을 구성합니다.

```cpp
// Device code
__global__ void MyKernel(int *array, int arrayCount)
{
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    if (idx < arrayCount) {
        array[idx] *= array[idx];
    }
}

// Host code
int launchMyKernel(int *array, int arrayCount)
{
    int blockSize;      // The launch configurator returned block size
    int minGridSize;    // The minimum grid size needed to achieve the
                        // maximum occupancy for a full device
                        // launch
    int gridSize;       // The actual grid size needed, based on input
                        // size

    cudaOccupancyMaxPotentialBlockSize(
        &minGridSize,
        &blockSize,
        (void*)MyKernel,
        0,
        arrayCount);

    // Round up according to array size
    gridSize = (arrayCount + blockSize - 1) / blockSize;

    MyKernel<<<gridSize, blockSize>>>(array, arrayCount);
    cudaDeviceSynchronize();

    // If interested, the occupancy can be calculated with
    // cudaOccupancyMaxActiveBlocksPerMultiprocessor

    return 0;
}
```

다음 코드 샘플은 주어진 크기의 활성 클러스터 수를 찾기 위해 클러스터 점유율 API를 사용하는 방법을 보여줍니다. 아래의 예제 코드는 크기 2의 클러스터와 블록당 128개의 스레드에 대한 점유율을 계산합니다.

크기 8의 클러스터는 컴퓨트 능력 9.0부터 앞으로 호환 가능하지만, 8개의 멀티프로세서를 지원하기에는 너무 작은 GPU 하드웨어나 MIG 구성에서는 최대 클러스터 크기가 줄어듭니다. 그러나 클러스터 커널을 실행하기 전에 최대 클러스터 크기를 쿼리하는 것이 좋습니다. 최대 클러스터 크기는 `cudaOccupancyMaxPotentialClusterSize` API를 사용하여 쿼리할 수 있습니다.

```cpp
{
  cudaLaunchConfig_t config = {0};
  config.gridDim = number_of_blocks;
  config.blockDim = 128; // threads_per_block = 128
  config.dynamicSmemBytes = dynamic_shared_memory_size;

  cudaLaunchAttribute attribute[1];
  attribute[0].id = cudaLaunchAttributeClusterDimension;
  attribute[0].val.clusterDim.x = 2; // cluster_size = 2
  attribute[0].val.clusterDim.y = 1;
  attribute[0].val.clusterDim.z = 1;
  config.attrs = attribute;
  config.numAttrs = 1;

  int max_cluster_size = 0;
  cudaOccupancyMaxPotentialClusterSize(&max_cluster_size, (void *)kernel, &config);

  int max_active_clusters = 0;
  cudaOccupancyMaxActiveClusters(&max_active_clusters, (void *)kernel, &config);

  std::cout << "Max Active Clusters of size 2: " << max_active_clusters << std::endl;
}
```

CUDA Nsight Compute 사용자 인터페이스는 CUDA 소프트웨어 스택에 의존할 수 없는 모든 사용 사례를 위해 `<CUDA_Toolkit_Path>/include/cuda_occupancy.h`에 독립 실행형 점유율 계산기 및 런치 구성기 구현을 제공합니다. Nsight Compute 버전의 점유율 계산기는 점유율에 영향을 미치는 매개변수(블록 크기, 스레드당 레지스터 수 및 스레드당 공유 메모리)의 변경 영향을 시각화하는 학습 도구로 특히 유용합니다.