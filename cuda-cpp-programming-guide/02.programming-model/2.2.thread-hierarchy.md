## 2.2. 스레드 계층 구조

편의를 위해 `threadIdx`는 3개의 구성 요소로 이루어진 벡터입니다. 이를 통해 스레드는 1차원, 2차원 또는 3차원 스레드 인덱스를 사용하여 식별될 수 있으며, 이러한 스레드 인덱스는 스레드 블록(thread block)을 형성합니다. 이는 벡터, 행렬 또는 볼륨과 같은 도메인 내의 요소에 대한 계산을 자연스럽게 호출할 수 있는 방법을 제공합니다.

스레드의 인덱스와 스레드 ID는 다음과 같은 간단한 방법으로 서로 관련되어 있습니다: 1차원 블록의 경우, 동일합니다; 크기가 $(Dx, Dy)$인 2차원 블록의 경우, 인덱스 $(x, y)$의 스레드 ID는 $(x + y Dx)$입니다; 크기가 $(Dx, Dy, Dz)$인 3차원 블록의 경우, 인덱스 $(x, y, z)$의 스레드 ID는 $(x + y Dx + z Dx Dy)$입니다.

예를 들어, 다음 코드는 크기가 $N \times N$인 두 개의 행렬 $A$와 $B$를 더하고 결과를 행렬 $C$에 저장합니다.

```cpp
// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
                       float C[N][N])
{
    int i = threadIdx.x;
    int j = threadIdx.y;
    C[i][j] = A[i][j] + B[i][j];
}

int main()
{
    ...
    // Kernel invocation with one block of N * N * 1 threads
    int numBlocks = 1;
    dim3 threadsPerBlock(N, N);
    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
    ...
}
```

스레드 블록 내의 스레드 수에는 제한이 있습니다. 모든 스레드 블록의 스레드는 동일한 스트리밍 다중 프로세서 코어에 있어야 하며, 해당 코어의 제한된 메모리 리소스를 공유해야 합니다. **현재의 GPU에서는 스레드 블록에 최대 1024개의 스레드가 포함될 수 있습니다.**

커널은 여러 개의 동일한 모양의 스레드 블록에 의해 실행될 수 있으며, 이는 스레드 블록의 수와 스레드 블록 내의 스레드 수의 곱과 같은 총 스레드 수를 갖습니다.

스레드 블록은 1차원, 2차원 또는 3차원 스레드 블록 그리드로 구성됩니다(그림 4 참조). 그리드 내의 스레드 블록 수는 처리되는 데이터의 크기에 의해 결정되며, 이는 일반적으로 시스템의 프로세서 수를 초과하는 경향이 있습니다.

![그림 4 스레드 블록 그리드](https://docs.nvidia.com/cuda/cuda-c-programming-guide/_images/grid-of-thread-blocks.png){: .align-center}
*그림 4 스레드 블록 그리드*

스레드 블록 수와 그리드 내의 스레드 블록 수를 지정하는 `<<<...>>>` 구문에서 사용되는 스레드 수와 블록 수는 `int` 또는 `dim3` 유형일 수 있습니다. 2차원 블록 또는 그리드는 위의 예제와 같이 지정할 수 있습니다.

그리드 내의 각 블록은 하나의 1차원, 2차원 또는 3차원 고유 인덱스로 식별될 수 있으며, 이는 내장된 `blockIdx` 변수를 통해 커널 내에서 액세스할 수 있습니다. 스레드 블록의 차원은 내장된 `blockDim` 변수를 통해 커널 내에서 액세스할 수 있습니다.

이전의 `MatAdd()` 예제를 여러 블록을 처리할 수 있도록 확장하면 다음과 같은 코드가 됩니다.

```cpp
// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
float C[N][N])
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
    if (i < N && j < N)
        C[i][j] = A[i][j] + B[i][j];
}

int main()
{
    ...
    // Kernel invocation
    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);
    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
    ...
}
```

스레드 블록 크기가 16x16 (256 스레드)이지만 이 경우 임의로 선택되었으며, 일반적으로 선택되는 크기입니다. 그리드는 이전과 같이 각 행렬 요소당 하나의 스레드를 가지도록 충분한 블록으로 생성됩니다. 간단히 하기 위해, 이 예제는 각 차원에서 그리드 내의 스레드 수가 해당 차원에서 블록 내의 스레드 수로 나누어 떨어지는 경우를 가정합니다.

**스레드 블록은 독립적으로 실행되어야 합니다.** 블록은 어떤 순서로든, 병렬 또는 순차적으로 실행될 수 있어야 합니다. 이 독립성 요구 사항은 프로그래머가 코어 수에 따라 확장되는 코드를 작성할 수 있도록 하여 그림 3에 나타난 것처럼 블록을 임의의 순서로 예약하고 임의의 수의 코어에 걸쳐 실행할 수 있도록 합니다.

스레드 블록 내의 스레드는 공유 메모리를 통해 데이터를 공유하고 실행을 동기화하여 메모리 액세스를 조정할 수 있습니다. 더 정확히 말하면, 커널 내에서 `__syncthreads()` 내장 함수를 호출하여 동기화 지점을 지정할 수 있습니다; `__syncthreads()`는 블록 내의 모든 스레드가 이전에 진행할 수 있도록 하기 전에 모든 스레드가 대기해야 하는 바리어(스레드들이 특정 지점에 도달할 때까지 대기하도록 하는 동기화 지점)입니다. [공유 메모리](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#shared-memory)는 공유 메모리를 사용하는 예제를 제공합니다. `__syncthreads()` 외에도, [Cooperative Groups API](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#cooperative-groups)는 협업 그룹 API를 제공합니다.

효율적인 협업을 위해, 공유 메모리는 각 프로세서 코어 근처에 위치한 낮은 레이턴시 메모리(예: L1 캐시와 유사)여야 하며, `__syncthreads()`는 경량화되어야 합니다.

### 2.2.1. 스레드 블록 클러스터

NVIDIA Compute Capability 9.0에서 CUDA 프로그래밍 모델은 스레드 블록 클러스터라는 선택적 계층을 도입합니다. 이는 스레드 블록으로 구성됩니다. 스레드 블록 내의 스레드가 스트리밍 다중 프로세서에서 동시에 실행되는 것처럼, 클러스터 내의 스레드 블록도 GPU 프로세싱 클러스터(GPC)에서 동시에 실행되도록 보장합니다.

클러스터는 또한 그림 5에 나타난 것처럼 1차원, 2차원 또는 3차원 스레드 블록 클러스터 그리드로 구성됩니다. 클러스터 내의 스레드 블록 수는 사용자 정의가 가능하며, CUDA에서 지원하는 휴대 가능한 클러스터 크기는 최대 8개의 스레드 블록입니다. 8개의 멀티프로세서를 지원하기에는 너무 작은 GPU 하드웨어나 MIG 구성에서는 최대 클러스터 크기가 그에 따라 줄어들게 됩니다. 이러한 작은 구성과 8개를 초과하는 스레드 블록 클러스터 크기를 지원하는 더 큰 구성의 식별은 아키텍처에 따라 다르며, `cudaOccupancyMaxPotentialClusterSize` API를 사용하여 쿼리할 수 있습니다.

![그림 5 스레드 블록 클러스터 그리드](https://docs.nvidia.com/cuda/cuda-c-programming-guide/_images/grid-of-clusters.png){: .align-center}
*그림 5 스레드 블록 클러스터 그리드*

> 주의
> 클러스터 지원을 사용하여 실행된 커널에서는 `gridDim` 변수가 호환성 목적을 위해 여전히 스레드 블록의 수에 따른 크기를 나타냅니다. 클러스터 내의 블록 순위는 [클러스터 그룹](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#cluster-group-cg) API를 사용하여 확인할 수 있습니다.

스레드 블록 클러스터는 커널에서 컴파일 시간 커널 속성 `__cluster_dims__(X,Y,Z)`를 사용하거나 CUDA 커널 런치 API `cudaLaunchKernelEx`를 사용하여 활성화할 수 있습니다. 아래 예시는 컴파일 시간 커널 속성을 사용하여 클러스터를 실행하는 방법을 보여줍니다. 커널 속성을 사용하여 설정된 클러스터 크기는 컴파일 시간에 고정되며, 이후 클래식 `<<< , >>>` 구문을 사용하여 커널을 실행할 수 있습니다. 커널이 컴파일 시간 클러스터 크기를 사용하는 경우, 클러스터 크기는 커널을 실행할 때 수정할 수 없습니다.

```cpp
// Kernel definition
// Compile time cluster size 2 in X-dimension and 1 in Y and Z dimension
__global__ void __cluster_dims__(2, 1, 1) cluster_kernel(float *input, float* output)
{

}

int main()
{
    float *input, *output;
    // Kernel invocation with compile time cluster size
    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);

    // The grid dimension is not affected by cluster launch, and is still enumerated
    // using number of blocks.
    // The grid dimension must be a multiple of cluster size.
    cluster_kernel<<<numBlocks, threadsPerBlock>>>(input, output);
}
```

스레드 블록 클러스터 크기는 런타임에도 설정할 수 있으며, CUDA 커널 실행 API `cudaLaunchKernelEx`를 사용하여 커널을 실행할 수 있습니다. 아래 코드 예제는 확장 가능한 API를 사용하여 클러스터 커널을 실행하는 방법을 보여줍니다.

```cpp
// Kernel definition
// No compile time attribute attached to the kernel
__global__ void cluster_kernel(float *input, float* output)
{

}

int main()
{
    float *input, *output;
    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);

    // Kernel invocation with runtime cluster size
    {
        cudaLaunchConfig_t config = {0};
        // The grid dimension is not affected by cluster launch, and is still enumerated
        // using number of blocks.
        // The grid dimension should be a multiple of cluster size.
        config.gridDim = numBlocks;
        config.blockDim = threadsPerBlock;

        cudaLaunchAttribute attribute[1];
        attribute[0].id = cudaLaunchAttributeClusterDimension;
        attribute[0].val.clusterDim.x = 2; // Cluster size in X-dimension
        attribute[0].val.clusterDim.y = 1;
        attribute[0].val.clusterDim.z = 1;
        config.attrs = attribute;
        config.numAttrs = 1;

        cudaLaunchKernelEx(&config, cluster_kernel, input, output);
    }
}
```

NVIDIA Compute Capability 9.0에서 클러스터 내의 모든 스레드 블록은 단일 GPU 프로세싱 클러스터(GPC)에서 동시에 실행되도록 보장되며, 클러스터 그룹 API인 `cluster.sync()`를 사용하여 하드웨어 지원 동기화를 수행할 수 있습니다. 클러스터 그룹은 각각 `num_threads()` 및 `num_blocks()` API를 사용하여 스레드 수 또는 블록 수에 따른 클러스터 그룹 크기를 쿼리할 수 있는 멤버 함수를 제공합니다. 클러스터 그룹 내의 스레드 또는 블록의 순위는 각각 `dim_threads()` 및 `dim_blocks()` API를 사용하여 쿼리할 수 있습니다.

클러스터에 속한 스레드 블록은 분산 공유 메모리에 액세스할 수 있습니다. 클러스터에 속한 스레드 블록은 분산 공유 메모리에서 읽기, 쓰기 및 원자 연산을 수행할 수 있습니다. 분산 공유 메모리는 [분산 공유 메모리](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#distributed-shared-memory)에서 히스토그램을 수행하는 예제를 제공합니다.
