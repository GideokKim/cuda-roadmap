## 2.4. 이종 프로그래밍

그림 7에 나타난 것처럼, CUDA 프로그래밍 모델은 CUDA 스레드가 C++ 프로그램을 실행하는 호스트에 대한 보조 프로세서로 작동하는 물리적으로 분리된 장치에서 실행된다고 가정합니다. 예를 들어, 커널이 GPU에서 실행되고 나머지 C++ 프로그램이 CPU에서 실행되는 경우가 이에 해당합니다.

CUDA 프로그래밍 모델은 또한 호스트와 디바이스가 각각 *호스트 메모리*와 *디바이스 메모리*라고 하는 DRAM 내의 별도의 메모리 공간을 유지한다고 가정합니다. 따라서 프로그램은 CUDA 런타임([프로그래밍 인터페이스](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#programming-interface)에서 설명됨)에 대한 호출을 통해 커널에 보이는 전역, 상수 및 텍스처 메모리 공간을 관리합니다. 여기에는 디바이스 메모리 할당 및 해제, 호스트와 디바이스 메모리 간의 데이터 전송이 포함됩니다.

통합 메모리(Unified Memory)는 호스트와 장치 메모리 공간을 연결하는 *관리된 메모리(Managed Memory)*를 제공합니다. 관리된 메모리는 시스템의 모든 CPU와 GPU에서 단일 일관된 메모리 이미지로 접근할 수 있으며, 공통 주소 공간을 가집니다. 이 기능은 장치 메모리의 과잉 구독을 가능하게 하며, 호스트와 장치 간의 데이터를 명시적으로 미러링할 필요를 없애어 애플리케이션 이식 작업을 크게 단순화할 수 있습니다. 통합 메모리에 대한 소개는 [통합 메모리 프로그래밍](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#um-unified-memory-programming-hd)을 참조하십시오.

![이종 프로그래밍](https://docs.nvidia.com/cuda/cuda-c-programming-guide/_images/heterogeneous-programming.png)
*그림 7 이종 프로그래밍*

> 주의
> 직렬 코드는 호스트에서 실행되고 병렬 코드는 장치에서 실행됩니다.